# -*- coding: utf-8 -*-
"""Pattern Assignment 8 LSTM with keras.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j5P8FX0oGU1WlDCdSa5u0eSRB-qSCN0A

recurrent neural network -----
IMDB movie review sentiment classification

import important libraries
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import tensorflow as tf

from tensorflow import keras
from keras.utils import normalize,to_categorical
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.datasets import imdb
from keras.models import Model
from keras.layers import Flatten, GlobalMaxPooling1D, Embedding, Conv1D, LSTM,Activation, Dropout, Dense,Bidirectional, Input

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,classification_report

"""intialize fixed variables to our model"""

MAX_SEQUENCE_LENGTH = 500  # all text-sequences are padded to this length
MAX_NB_WORDS = 10000        # number of most-frequent words that are regarded, all others are ignored
EMBEDDING_DIM = 100         # dimension of word-embedding
INDEX_FROM=3

"""load dataset"""

(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=MAX_NB_WORDS,index_from=INDEX_FROM)

print(X_train)

print(y_train)

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

"""preprocessing"""

X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)

X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)

y_train = to_categorical(np.asarray(y_train))
y_test = to_categorical(np.asarray(y_test))
print('Shape of Training Input:', X_train.shape)
print('Shape of Training Labels:', y_train.shape)

print('Number of positive and negative reviews in training and validation set ')
print (y_train.sum(axis=0))
print (y_test.sum(axis=0))

"""build LSTM model"""

embedding_layer = Embedding(MAX_NB_WORDS,
                            EMBEDDING_DIM,
                            input_length=MAX_SEQUENCE_LENGTH,
                            trainable=True)

sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')
embedded_sequences = embedding_layer(sequence_input)
l_lstm = Bidirectional(LSTM(64))(embedded_sequences)
preds = Dense(2, activation='softmax')(l_lstm)
model = Model(sequence_input, preds)

"""print  model summary"""

model.summary()

"""compile the model"""

model.compile(loss='categorical_crossentropy', optimizer='rmsprop',metrics=['accuracy'])

"""fit the model"""

history=model.fit(X_train, y_train, validation_data=(X_test, y_test),epochs=10, batch_size=128)

"""accuracy"""

loss,acc=model.evaluate(X_test,y_test)
print('Total Accuracy',acc)